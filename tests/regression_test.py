import pytest
import subprocess
import scona as scn
import scona.datasets as datasets
import hashlib
import shutil
import sys
import os
import json


def critical_routine(output_dir):
    """
    This is the function we are testing in our regression test.
    We will run it first at a stage when we are confident that 
    the results are accurate. We store the results of that run 
    to compare later runs to.
    """
    # import the Whitaker_vertes dataset
    regionalmeasures, names, covars, centroids = (
        datasets.NSPN_WhitakerVertes_PNAS2016._data())
    
    subprocess.run([
        "scona",
        "standard_analysis",
        regionalmeasures,
        centroids,
        names,
        "--output_dir",
        output_dir,
        "-s 2984",
        "-n 10",
        "--output_name",
        "corrmat.txt"
    ])

# For convenience and to save on space we will store a hash of
# the fixture output, instead of retaining the whole directory

def hash_file(filename):
    """
    Reads in file and hashes contents
    """
    m = hashlib.sha256()
    with open(filename, 'rb') as f:
        while True:
            b = f.read(2**10)
            if not b:
                break
            m.update(b)
    return m.hexdigest()

def hash_folder(folder):
    """
    Walk through a directory and return a dictionary mapping
    file names to file hashes. 

    n.b. we could could create just one hash for the whole
    folder, but when the regression test fails it will help
    to see which files it fails on.
    """
    hashes = {}
    for path, directories, files in os.walk(folder):
        for file in sorted(files):
            hashes[os.path.join(*directories, file)] = hash_file(
                os.path.join(path, file))
        for dir in sorted(directories):
            hashes.update(hash_folder(os.path.join(path, dir)))
        break
    return hashes

def generate_regression_hashes(test_routine, folder):
    """
    Run test_routine function and return a hash of the resulting
    folder. Deletes all files created by test_routine. Note that
    folder ought to be empty and/or not present at start.

    Parameters
    ----------
    test_routine : func
        A function, accepting the name of a directory as input.
        test_routine should populate this directory with some 
        output.
    folder : str
        A string, designating an empty or non-existent folder in
        which to store output of `test_routine` function.

    Returns 
    -------
    dict, str
        A dictionary recording the files generated by `test_routine`
        and their hashes.
        A string identifying the python version and function name
    """
    # First create a hash label. This can be empty or it can be used to
    # identify hashes when you will be creating more than one.
    # Here we will be recording the name of the function and the python
    # version during generation. As an example, if we were running
    # critical_routine in python 3.6 we would get "critical_routine3.6"
    hash_identifier = test_routine.__name__ + ".".join([str(sys.version_info[0]), str(sys.version_info[1])])
    # Now we run test_routine
    print("generating hash {}".format(hash_identifier))
    test_routine(folder)
    # create hash dictionary
    hash_dict = hash_folder(folder)
    # delete the folder created by test_routine
    print('\ndeleting temporary files')
    shutil.rmtree(os.path.join(os.getcwd(), folder))
    # return hash dictionary
    return hash_dict, hash_identifier

def store_fixture(hash_dict, hash_identifier):
    try:
        with open(
                os.path.join(os.path.dirname(__file__), ".fixture_hashes"),
                "r") as f:
            fixture_dict = json.load(f)
            fixture_dict.update({hash_identifier:hash_dict})
    except:
        fixture_dict = {hash_identifier: hash_dict}
    with open(os.path.join(os.path.dirname(__file__), ".fixture_hashes"), "w") as f:
        json.dump(fixture_dict, f)

        
        
def load_fixture(hash_identifier):
    """
    Unpickle tests/.fixture_hashing file and look for
    hashes with metadata matching hash_identifier
    """
    with open(os.path.join(os.path.dirname(__file__), ".fixture_hashes"), "r") as f:
        fixture_dict = json.load(f)
        try:
            return fixture_dict[hash_identifier]
        except KeyError:
            raise KeyError("no regression fixture found matching {}".format(hash_identifier))


# --------------------------- Tests --------------------------------
def test_old_hashes_against_new():
    # ------------------- setup and teardown ---------------------------
    print("generating temporary test files")
    hash_dict_new, hash_label = generate_regression_hashes(
        critical_routine, 'temporary_test_fixtures')

    print("loading old test fixtures")
    hash_dict_original = load_fixture(hash_label)
    
    for key in hash_dict_original.keys():
          print("testing regression on {}".format(key))
          assert hash_dict_new[key] == hash_dict_original[key]


if __name__ == '__main__':
    if (input("Are you sure you want to update scona's test fixtures? (y/n)")
            == 'y'):
        store_fixture(*generate_regression_hashes(critical_routine, "new_test_fixtures"))
